{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "This notebook aims to use AutoGluon to select the best model for the task of predicting if a customer will complete an offer given they view it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:44:30.067969Z",
     "start_time": "2023-05-19T16:42:55.105453Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogluon in /opt/conda/lib/python3.11/site-packages (1.2)\n",
      "Collecting bokeh==2.0.1\n",
      "  Downloading bokeh-2.0.1.tar.gz (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "  Preparing metadata (setup.py) ... \u001b[?25done\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (1.26.4)\n",
      "Requirement already satisfied: pillow>=4.0 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (11.1.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (24.2)\n",
      "Requirement already satisfied: tornado>=5 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (6.4.2)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4 in /opt/conda/lib/python3.11/site-packages (from bokeh==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: autogluon.core==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.core[all]==1.2->autogluon) (1.2)\n",
      "Requirement already satisfied: autogluon.features==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon) (1.2)\n",
      "Requirement already satisfied: autogluon.tabular==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.tabular[all]==1.2->autogluon) (1.2)\n",
      "Requirement already satisfied: autogluon.multimodal==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon) (1.2)\n",
      "Requirement already satisfied: autogluon.timeseries==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries[all]==1.2->autogluon) (1.2)\n",
      "Requirement already satisfied: scipy<1.16,>=1.5.4 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<1.5.3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.5.2)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.4.2)\n",
      "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.2.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (4.67.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.32.3)\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.10.1)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.36.23)\n",
      "Requirement already satisfied: autogluon.common==1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.2)\n",
      "Requirement already satisfied: ray<2.41,>=2.10.0 in /opt/conda/lib/python3.11/site-packages (from ray[default]<2.41,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.2->autogluon) (2.37.0)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/conda/lib/python3.11/site-packages (from autogluon.core[all]==1.2->autogluon) (0.2.7)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.core[all]==1.2->autogluon) (17.0.0)\n",
      "Requirement already satisfied: torch<2.6,>=2.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (2.4.1.post100)\n",
      "Requirement already satisfied: lightning<2.6,>=2.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (2.5.0.post0)\n",
      "Requirement already satisfied: transformers<5,>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (4.49.0)\n",
      "Requirement already satisfied: accelerate<1.0,>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.34.2)\n",
      "Collecting jsonschema<4.22,>=4.18 (from autogluon.multimodal==1.2->autogluon)\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: evaluate<0.5.0,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.4.1)\n",
      "Requirement already satisfied: timm<1.0.7,>=0.9.5 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.0.3)\n",
      "Requirement already satisfied: torchvision<0.21.0,>=0.16.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.19.1)\n",
      "Requirement already satisfied: scikit-image<0.25.0,>=0.19.1 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.20.0)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.3)\n",
      "Requirement already satisfied: torchmetrics<1.3.0,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.2.1)\n",
      "Collecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.2->autogluon)\n",
      "  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pytorch-metric-learning<2.4,>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (2.3.0)\n",
      "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.1.11)\n",
      "Collecting nltk<3.9,>=3.4.5 (from autogluon.multimodal==1.2->autogluon)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.9 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (2.17.1)\n",
      "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (0.3.10)\n",
      "Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.2->autogluon)\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.multimodal==1.2->autogluon) (1.17.0)\n",
      "Requirement already satisfied: catboost<1.3,>=1.2 in /opt/conda/lib/python3.11/site-packages (from autogluon.tabular[all]==1.2->autogluon) (1.2.7)\n",
      "Collecting einops<0.9,>=0.7 (from autogluon.tabular[all]==1.2->autogluon)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting lightgbm<4.6,>=4.0 (from autogluon.tabular[all]==1.2->autogluon)\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface_hub[torch] in /opt/conda/lib/python3.11/site-packages (from autogluon.tabular[all]==1.2->autogluon) (0.29.1)\n",
      "Requirement already satisfied: xgboost<2.2,>=1.6 in /opt/conda/lib/python3.11/site-packages (from autogluon.tabular[all]==1.2->autogluon) (2.1.4)\n",
      "Requirement already satisfied: fastai<2.8,>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from autogluon.tabular[all]==1.2->autogluon) (2.7.18)\n",
      "Collecting spacy<3.8 (from autogluon.tabular[all]==1.2->autogluon)\n",
      "  Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.4.2)\n",
      "Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.5.0.post0)\n",
      "Requirement already satisfied: gluonts<0.17,>=0.15.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: statsforecast<1.8,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.7.8)\n",
      "Requirement already satisfied: mlforecast==0.13.4 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.13.4)\n",
      "Requirement already satisfied: utilsforecast<0.2.5,>=0.2.3 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.2.3)\n",
      "Requirement already satisfied: coreforecast==0.0.12 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.0.12)\n",
      "Requirement already satisfied: fugue>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.9.1)\n",
      "Requirement already satisfied: orjson~=3.9 in /opt/conda/lib/python3.11/site-packages (from autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.10.15)\n",
      "Requirement already satisfied: psutil<7.0.0,>=5.7.3 in /opt/conda/lib/python3.11/site-packages (from autogluon.common==1.2->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (5.9.8)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.1.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2024.10.0)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.11/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.61.0)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (4.2.1)\n",
      "Requirement already satisfied: window-ops in /opt/conda/lib/python3.11/site-packages (from mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.0.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2>=2.7->bokeh==2.0.1) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.1->bokeh==2.0.1) (1.17.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate<1.0,>=0.34.0->autogluon.multimodal==1.2->autogluon) (0.5.3)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.23 in /opt/conda/lib/python3.11/site-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.36.23)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (0.11.3)\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.11/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (0.20.3)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (6.0.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (2.2.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.11/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.3.9)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.70.17)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.11/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (24.3.1)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.11/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /opt/conda/lib/python3.11/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (1.7.20)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.11/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.2->autogluon) (1.0.3)\n",
      "Requirement already satisfied: triad>=0.9.7 in /opt/conda/lib/python3.11/site-packages (from fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.9.8)\n",
      "Requirement already satisfied: adagio>=0.2.4 in /opt/conda/lib/python3.11/site-packages (from fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.2.6)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.11/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.10.6)\n",
      "Requirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.11/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.12.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.11/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.2->autogluon) (1.0.0)\n",
      "Requirement already satisfied: py4j in /opt/conda/lib/python3.11/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.2->autogluon) (0.10.9.9)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.2->autogluon) (0.23.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from lightning<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (0.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.2.1)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (5.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.2->autogluon) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.2->autogluon) (2024.11.6)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /opt/conda/lib/python3.11/site-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.2->autogluon) (4.9.3)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.11/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.4.6)\n",
      "Requirement already satisfied: model-index in /opt/conda/lib/python3.11/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.1.11)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (13.9.4)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.11/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (2025.1.31)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.11/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (2020.6.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (1.8.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.11/site-packages (from scikit-image<0.25.0,>=0.19.1->autogluon.multimodal==1.2->autogluon) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.core==1.2->autogluon.core[all]==1.2->autogluon) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.0.9)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (75.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (3.4.1)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.11/site-packages (from statsforecast<1.8,>=1.7.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.14.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (1.62.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (3.6)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (4.25.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.2->autogluon) (3.1.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (3.17.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (1.13.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers<5,>=4.38.0->transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (0.21.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.11/site-packages (from transformers[sentencepiece]<5,>=4.38.0->autogluon.multimodal==1.2->autogluon) (0.1.99)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (3.9.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (4.13.3)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.3.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.44.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.27.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.11/site-packages (from srsly<3.0.0,>=2.4.3->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (5.10.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.11/site-packages (from statsmodels>=0.13.2->statsforecast<1.8,>=1.7.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.0.1)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8->autogluon.tabular[all]==1.2->autogluon)\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.1.5)\n",
      "Requirement already satisfied: fs in /opt/conda/lib/python3.11/site-packages (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.4.16)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (2.19.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (7.1.0)\n",
      "Requirement already satisfied: ordered-set in /opt/conda/lib/python3.11/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (4.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (2.0.38)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.2->autogluon) (1.29.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch<2.6,>=2.2->autogluon.multimodal==1.2->autogluon) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (1.18.3)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.3.9)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.2->autogluon) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8->autogluon.tabular[all]==1.2->autogluon) (1.17.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna->mlforecast==0.13.4->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (3.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (2.5)\n",
      "Requirement already satisfied: appdirs~=1.4.3 in /opt/conda/lib/python3.11/site-packages (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.2->autogluon.timeseries[all]==1.2->autogluon) (1.4.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.2->autogluon) (1.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.2->autogluon) (0.2.1)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
      "Downloading spacy-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Building wheels for collected packages: bokeh, nvidia-ml-py3\n",
      "  Building wheel for bokeh done\n",
      "\u001b[?25h  Created wheel for bokeh: filename=bokeh-2.0.1-py3-none-any.whl size=9080076 sha256=9b6860e7110a70dba8cad3c174338cba0dcda26cd18be4ab63f3180077a1cf2f\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/0e/c4/ce/fecee9e6406e166eaba4e09b1acd2096a84ffef5275ea90806\n",
      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19206 sha256=5f9c01efe6759ce083863473dbf032ae5df2060497ceeb63c18cb239950233d1\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n",
      "Successfully built bokeh nvidia-ml-py3\n",
      "Installing collected packages: nvidia-ml-py3, omegaconf, nltk, einops, blis, lightgbm, bokeh, jsonschema, thinc, spacy\n",
      "  Attempting uninstall: omegaconf\n",
      "    Found existing installation: omegaconf 2.3.0\n",
      "    Uninstalling omegaconf-2.3.0:\n",
      "      Successfully uninstalled omegaconf-2.3.0\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "  Attempting uninstall: blis\n",
      "    Found existing installation: blis 1.0.1\n",
      "    Uninstalling blis-1.0.1:\n",
      "      Successfully uninstalled blis-1.0.1\n",
      "  Attempting uninstall: lightgbm\n",
      "    Found existing installation: lightgbm 4.6.0\n",
      "    Uninstalling lightgbm-4.6.0:\n",
      "      Successfully uninstalled lightgbm-4.6.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.23.0\n",
      "    Uninstalling jsonschema-4.23.0:\n",
      "      Successfully uninstalled jsonschema-4.23.0\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.3.2\n",
      "    Uninstalling thinc-8.3.2:\n",
      "      Successfully uninstalled thinc-8.3.2\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.8.2\n",
      "    Uninstalling spacy-3.8.2:\n",
      "      Successfully uninstalled spacy-3.8.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed blis-0.7.11 bokeh-2.0.1 einops-0.8.1 jsonschema-4.21.1 lightgbm-4.5.0 nltk-3.8.1 nvidia-ml-py3-7.352.0 omegaconf-2.2.3 spacy-3.7.5 thinc-8.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autogluon bokeh==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:00:12.452884Z",
     "start_time": "2023-05-19T21:00:10.588175Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-361673968127\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "session = sagemaker.Session() \n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:00:13.379644Z",
     "start_time": "2023-05-19T21:00:13.322686Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_days</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>gender_O</th>\n",
       "      <th>person</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>offer_viewed</th>\n",
       "      <th>...</th>\n",
       "      <th>reward</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>email</th>\n",
       "      <th>mobile</th>\n",
       "      <th>social</th>\n",
       "      <th>web</th>\n",
       "      <th>offer_bogo</th>\n",
       "      <th>offer_discount</th>\n",
       "      <th>offer_informational</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.0</td>\n",
       "      <td>2017-07-15</td>\n",
       "      <td>112000.0</td>\n",
       "      <td>376</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0610b486422d4921ae7d2bf64640c50b</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>ae264e3637204a6fb9bb56bc8210ddfd</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>2017-05-09</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47011</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>871</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9dc1421481194dcd9400aec7c9ae6366</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47012</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>871</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9dc1421481194dcd9400aec7c9ae6366</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47013</th>\n",
       "      <td>83.0</td>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>871</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>9dc1421481194dcd9400aec7c9ae6366</td>\n",
       "      <td>ae264e3637204a6fb9bb56bc8210ddfd</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47014</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>369</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>e4052622e5ba45a8b96b59aba68cf068</td>\n",
       "      <td>2298d6c36e964ae4a3e7e9706d1fb8c2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47015</th>\n",
       "      <td>62.0</td>\n",
       "      <td>2017-07-22</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>369</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>e4052622e5ba45a8b96b59aba68cf068</td>\n",
       "      <td>f19421c1d4aa40978ebb69ca19b0e20d</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47016 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age became_member_on    income  membership_days  gender_F  gender_M  \\\n",
       "0      55.0       2017-07-15  112000.0              376      True     False   \n",
       "1      75.0       2017-05-09  100000.0              443      True     False   \n",
       "2      75.0       2017-05-09  100000.0              443      True     False   \n",
       "3      75.0       2017-05-09  100000.0              443      True     False   \n",
       "4      75.0       2017-05-09  100000.0              443      True     False   \n",
       "...     ...              ...       ...              ...       ...       ...   \n",
       "47011  83.0       2016-03-07   50000.0              871      True     False   \n",
       "47012  83.0       2016-03-07   50000.0              871      True     False   \n",
       "47013  83.0       2016-03-07   50000.0              871      True     False   \n",
       "47014  62.0       2017-07-22   82000.0              369      True     False   \n",
       "47015  62.0       2017-07-22   82000.0              369      True     False   \n",
       "\n",
       "       gender_O                            person  \\\n",
       "0         False  0610b486422d4921ae7d2bf64640c50b   \n",
       "1         False  78afa995795e4d85b5d9ceeca43f5fef   \n",
       "2         False  78afa995795e4d85b5d9ceeca43f5fef   \n",
       "3         False  78afa995795e4d85b5d9ceeca43f5fef   \n",
       "4         False  78afa995795e4d85b5d9ceeca43f5fef   \n",
       "...         ...                               ...   \n",
       "47011     False  9dc1421481194dcd9400aec7c9ae6366   \n",
       "47012     False  9dc1421481194dcd9400aec7c9ae6366   \n",
       "47013     False  9dc1421481194dcd9400aec7c9ae6366   \n",
       "47014     False  e4052622e5ba45a8b96b59aba68cf068   \n",
       "47015     False  e4052622e5ba45a8b96b59aba68cf068   \n",
       "\n",
       "                               offer_id offer_viewed  ... reward  difficulty  \\\n",
       "0      9b98b8c7a33c4b65b9aebfe6a799e6d9        False  ...    5.0         5.0   \n",
       "1      5a8bc65990b245e5a138643cd4eb9837         True  ...    0.0         0.0   \n",
       "2      9b98b8c7a33c4b65b9aebfe6a799e6d9         True  ...    5.0         5.0   \n",
       "3      ae264e3637204a6fb9bb56bc8210ddfd         True  ...   10.0        10.0   \n",
       "4      f19421c1d4aa40978ebb69ca19b0e20d         True  ...    5.0         5.0   \n",
       "...                                 ...          ...  ...    ...         ...   \n",
       "47011  4d5c57ea9a6940dd891ad53e9dbe8da0         True  ...   10.0        10.0   \n",
       "47012  9b98b8c7a33c4b65b9aebfe6a799e6d9         True  ...    5.0         5.0   \n",
       "47013  ae264e3637204a6fb9bb56bc8210ddfd         True  ...   10.0        10.0   \n",
       "47014  2298d6c36e964ae4a3e7e9706d1fb8c2         True  ...    3.0         7.0   \n",
       "47015  f19421c1d4aa40978ebb69ca19b0e20d         True  ...    5.0         5.0   \n",
       "\n",
       "       duration  email  mobile  social  web  offer_bogo offer_discount  \\\n",
       "0           7.0    1.0     1.0     0.0  1.0        True          False   \n",
       "1           3.0    1.0     1.0     1.0  0.0       False          False   \n",
       "2           7.0    1.0     1.0     0.0  1.0        True          False   \n",
       "3           7.0    1.0     1.0     1.0  0.0        True          False   \n",
       "4           5.0    1.0     1.0     1.0  1.0        True          False   \n",
       "...         ...    ...     ...     ...  ...         ...            ...   \n",
       "47011       5.0    1.0     1.0     1.0  1.0        True          False   \n",
       "47012       7.0    1.0     1.0     0.0  1.0        True          False   \n",
       "47013       7.0    1.0     1.0     1.0  0.0        True          False   \n",
       "47014       7.0    1.0     1.0     1.0  1.0       False           True   \n",
       "47015       5.0    1.0     1.0     1.0  1.0        True          False   \n",
       "\n",
       "      offer_informational  \n",
       "0                   False  \n",
       "1                    True  \n",
       "2                   False  \n",
       "3                   False  \n",
       "4                   False  \n",
       "...                   ...  \n",
       "47011               False  \n",
       "47012               False  \n",
       "47013               False  \n",
       "47014               False  \n",
       "47015               False  \n",
       "\n",
       "[47016 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset = pd.read_feather('./data/processed/processed_dataset.feather')\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T21:00:13.934160Z",
     "start_time": "2023-05-19T21:00:13.907041Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan features in target: 0\n",
      "Training set: (32855, 17)\n",
      "Validation set: (7040, 17)\n",
      "Test set: (7041, 17)\n"
     ]
    }
   ],
   "source": [
    "target_column = 'offer_completed_after_view'\n",
    "non_train_features = ['became_member_on', 'person', 'offer_id', 'offer_viewed']\n",
    "\n",
    "# Remove Nan features from dataset\n",
    "print(f'Nan features in target: {processed_dataset[target_column].isna().sum()}')\n",
    "processed_dataset = processed_dataset[processed_dataset[target_column].notna()]\n",
    "processed_dataset[target_column] = processed_dataset[target_column].astype(bool)\n",
    "\n",
    "# Remove features not able to use for train\n",
    "processed_dataset = processed_dataset.drop(columns=non_train_features, axis=1)\n",
    "\n",
    "# Set the target column as the first since it is how Sagemaker training expects it\n",
    "column_order = [target_column] + [col for col in processed_dataset.columns if col != target_column]\n",
    "processed_dataset = processed_dataset[column_order]\n",
    "\n",
    "# Define the train, validation and test size ratios\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Split the dataset into train (70%) and test (30%)\n",
    "train_dataset, temp_dataset = train_test_split(processed_dataset, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "# Calculate the size ratio of validation and test sets\n",
    "val_test_ratio = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "# Split the remaining dataset (X_temp, y_temp) into validation (15%) and test (15%)\n",
    "val_dataset, test_dataset = train_test_split(temp_dataset, test_size=val_test_ratio, random_state=42)\n",
    "\n",
    "print('Training set:', train_dataset.shape)\n",
    "print('Validation set:', val_dataset.shape)\n",
    "print('Test set:', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['offer_completed_after_view', 'age', 'income', 'membership_days', 'gender_F', 'gender_M', 'gender_O', 'reward', 'difficulty', 'duration', 'email', 'mobile', 'social', 'web', 'offer_bogo', 'offer_discount', 'offer_informational']\n"
     ]
    }
   ],
   "source": [
    "# Save datases in S3\n",
    "prefix = 'data'\n",
    "data_dir = './data/processed'\n",
    "\n",
    "train_dataset_path = os.path.join(data_dir, 'train.csv')\n",
    "val_dataset_path = os.path.join(data_dir, 'validation.csv')\n",
    "test_dataset_path = os.path.join(data_dir, 'test.csv')\n",
    "\n",
    "train_dataset.to_csv(train_dataset_path, index=False, header=False)\n",
    "val_dataset.to_csv(val_dataset_path, index=False, header=False)\n",
    "test_dataset.to_csv(test_dataset_path, index=False, header=False)\n",
    "\n",
    "# Upload the test.csv, train.csv and validation.csv files which are contained in data_dir to S3 using sess.upload_data().\n",
    "train_location = session.upload_data(train_dataset_path, key_prefix=prefix)\n",
    "val_location = session.upload_data(val_dataset_path, key_prefix=prefix)\n",
    "test_location = session.upload_data(test_dataset_path, key_prefix=prefix)\n",
    "\n",
    "train_location, val_location, test_location\n",
    "\n",
    "# Save columns since we will not have them available in S3\n",
    "print(train_dataset.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250329_192515\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Feb 14 16:52:40 UTC 2025\n",
      "CPU Count:          2\n",
      "Memory Avail:       1.06 GB / 3.76 GB (28.3%)\n",
      "Disk Space Avail:   4.81 GB / 4.99 GB (96.4%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to False. Reason: Skip dynamic_stacking when use_bag_holdout is enabled. (use_bag_holdout=True)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/home/sagemaker-user/AutogluonModels/ag-20250329_192515\"\n",
      "Train Data Rows:    32855\n",
      "Train Data Columns: 16\n",
      "Tuning Data Rows:    7040\n",
      "Tuning Data Columns: 16\n",
      "Label Column:       offer_completed_after_view\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = True, class 0 = False\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1094.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.27 MB (0.7% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 9 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['email']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])   : 3 | ['gender_F', 'gender_M', 'gender_O']\n",
      "\t\t('float', [])  : 8 | ['age', 'income', 'reward', 'difficulty', 'duration', ...]\n",
      "\t\t('int', [])    : 1 | ['membership_days']\n",
      "\t\t('object', []) : 3 | ['offer_bogo', 'offer_discount', 'offer_informational']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 5 | ['age', 'income', 'reward', 'difficulty', 'duration']\n",
      "\t\t('int', [])       : 1 | ['membership_days']\n",
      "\t\t('int', ['bool']) : 9 | ['gender_F', 'gender_M', 'gender_O', 'mobile', 'social', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t15 features in original data used to generate 15 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.17 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'average_precision'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "use_bag_holdout=True, will use tuning_data as holdout (will not be used for early stopping).\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1199.49s of the 1799.68s of remaining time.\n",
      "\t0.4129\t = Validation score   (average_precision)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1195.08s of the 1795.26s of remaining time.\n",
      "\t0.4429\t = Validation score   (average_precision)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1194.58s of the 1794.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.64%)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=750, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=750, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1180.42s of the 1780.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.49%)\n",
      "2025-03-29 19:25:38,934\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=751, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\tWarning: Exception caused LightGBM_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=881, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=881, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1171.65s of the 1771.83s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 57 due to low memory. Expected memory usage reduced from 78.2% -> 15.0% of available memory...\n",
      "\t0.5804\t = Validation score   (average_precision)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1167.76s of the 1767.94s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 78 due to low memory. Expected memory usage reduced from 57.0% -> 15.0% of available memory...\n",
      "2025-03-29 19:25:48,936\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t0.5811\t = Validation score   (average_precision)\n",
      "\t3.62s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1163.17s of the 1763.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.26%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: acf3611cf376c199bfd0eba153e4d3c7e19229c701000000, name=_ray_fit, pid=1348, memory used=0.17GB) was running was 3.57GB / 3.76GB (0.95043), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 43eea0ab78c341f146f4ee7a76c392218d75077621381e96747d5903) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-43eea0ab78c341f146f4ee7a76c392218d75077621381e96747d5903*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1280\t0.25\tray::IDLE\n",
      "1348\t0.17\tray::_ray_fit\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: acf3611cf376c199bfd0eba153e4d3c7e19229c701000000, name=_ray_fit, pid=1348, memory used=0.17GB) was running was 3.57GB / 3.76GB (0.95043), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 43eea0ab78c341f146f4ee7a76c392218d75077621381e96747d5903) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-43eea0ab78c341f146f4ee7a76c392218d75077621381e96747d5903*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1280\t0.25\tray::IDLE\n",
      "1348\t0.17\tray::_ray_fit\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1104.21s of the 1704.40s of remaining time.\n",
      "\tWarning: Model is expected to require 199.98% of available memory...\n",
      "\tNot enough memory to train ExtraTreesGini_BAG_L1... Skipping this model.\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1103.33s of the 1703.51s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 41 due to low memory. Expected memory usage reduced from 107.92% -> 15.0% of available memory...\n",
      "\t0.582\t = Validation score   (average_precision)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1101.05s of the 1701.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.21%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: c162496e18817096cc6f6d60710ed2fec6fb2cf801000000, name=_ray_fit, pid=1759, memory used=0.14GB) was running was 3.58GB / 3.76GB (0.952567), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9793b807b6e2ef9c3b951da945870041c5b1d51d026ace034a79f1b4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-9793b807b6e2ef9c3b951da945870041c5b1d51d026ace034a79f1b4*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1759\t0.14\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: c162496e18817096cc6f6d60710ed2fec6fb2cf801000000, name=_ray_fit, pid=1759, memory used=0.14GB) was running was 3.58GB / 3.76GB (0.952567), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9793b807b6e2ef9c3b951da945870041c5b1d51d026ace034a79f1b4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-9793b807b6e2ef9c3b951da945870041c5b1d51d026ace034a79f1b4*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1759\t0.14\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1039.65s of the 1639.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.66%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 1c26ea729ee16985796cb43cdf7e1e0aa13ba59f01000000, name=_ray_fit, pid=1789, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.951998), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0a36fb6eab0d5e6ed9ecd40834f4b1ba1533339b23b91260f4b1ea72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-0a36fb6eab0d5e6ed9ecd40834f4b1ba1533339b23b91260f4b1ea72*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "1789\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 1c26ea729ee16985796cb43cdf7e1e0aa13ba59f01000000, name=_ray_fit, pid=1789, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.951998), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0a36fb6eab0d5e6ed9ecd40834f4b1ba1533339b23b91260f4b1ea72) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-0a36fb6eab0d5e6ed9ecd40834f4b1ba1533339b23b91260f4b1ea72*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "1789\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1037.96s of the 1638.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.45%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 2fd7006bb963b08122c867b090a7090d2ca4312b01000000, name=_ray_fit, pid=1841, memory used=0.06GB) was running was 3.57GB / 3.76GB (0.950587), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9231fb22408190382053eba1ba79e8435a2ce0f67499ed283b19d182) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-9231fb22408190382053eba1ba79e8435a2ce0f67499ed283b19d182*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "1841\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 2fd7006bb963b08122c867b090a7090d2ca4312b01000000, name=_ray_fit, pid=1841, memory used=0.06GB) was running was 3.57GB / 3.76GB (0.950587), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 9231fb22408190382053eba1ba79e8435a2ce0f67499ed283b19d182) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-9231fb22408190382053eba1ba79e8435a2ce0f67499ed283b19d182*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "1382\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "1841\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-03-29 19:27:59,251\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1035.86s of the 1636.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.96%)\n",
      "2025-03-29 19:28:04,093\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=1960, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=1960, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1026.13s of the 1626.32s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=8.43%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 8ded0ffb5de73e281a71b6afc40c360a688c466201000000, name=_ray_fit, pid=2033, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954395), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a088d014c95c6cb40b2dc47ac2f38fb0b4b0a81897a043fdbcc2a2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-8a088d014c95c6cb40b2dc47ac2f38fb0b4b0a81897a043fdbcc2a2b*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2033\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 8ded0ffb5de73e281a71b6afc40c360a688c466201000000, name=_ray_fit, pid=2033, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954395), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 8a088d014c95c6cb40b2dc47ac2f38fb0b4b0a81897a043fdbcc2a2b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-8a088d014c95c6cb40b2dc47ac2f38fb0b4b0a81897a043fdbcc2a2b*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2033\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-03-29 19:28:12,822\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=1892, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1022.28s of the 1622.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=8.82%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ec734d083361b13b924883927cf40b05c49e35a201000000, name=_ray_fit, pid=2098, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954866), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c3a62b6919f1d0a1a779a54ac4f79a2d4ced84f78034a6003bfa9aea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c3a62b6919f1d0a1a779a54ac4f79a2d4ced84f78034a6003bfa9aea*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2098\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ec734d083361b13b924883927cf40b05c49e35a201000000, name=_ray_fit, pid=2098, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954866), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c3a62b6919f1d0a1a779a54ac4f79a2d4ced84f78034a6003bfa9aea) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c3a62b6919f1d0a1a779a54ac4f79a2d4ced84f78034a6003bfa9aea*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2098\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1019.95s of the 1620.13s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.07% memory usage per fold, 44.27%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=11.07%)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a9431de745641d7800028402132c72e35a054df701000000, name=_ray_fit, pid=2163, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954031), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3a53ce59af610b6721d0efccc96d6000f8530baae165766294d2f336) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-3a53ce59af610b6721d0efccc96d6000f8530baae165766294d2f336*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2163\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a9431de745641d7800028402132c72e35a054df701000000, name=_ray_fit, pid=2163, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954031), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 3a53ce59af610b6721d0efccc96d6000f8530baae165766294d2f336) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-3a53ce59af610b6721d0efccc96d6000f8530baae165766294d2f336*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "1892\t0.19\tray::IDLE\n",
      "1960\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "2163\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1017.68s of the 1617.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.23%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a87d215ce6faa5e40566fd1848bcd1d99f6b936701000000, name=_ray_fit, pid=4269, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.951452), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c66c4674b455160706835d44692966466911a6b3006ff56b452cc16a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c66c4674b455160706835d44692966466911a6b3006ff56b452cc16a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "3807\t0.33\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "4269\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a87d215ce6faa5e40566fd1848bcd1d99f6b936701000000, name=_ray_fit, pid=4269, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.951452), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c66c4674b455160706835d44692966466911a6b3006ff56b452cc16a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c66c4674b455160706835d44692966466911a6b3006ff56b452cc16a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "3807\t0.33\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "4269\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 607.45s of the 1207.63s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.70% memory usage per fold, 58.81%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=14.70%)\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 5f8470ab97b09fc9e8814a6429a9c9f0c46a8b8c01000000, name=_ray_fit, pid=4268, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.952923), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 45079c919740e19b82b941714aa4ccc383a112ca8df30778c06f3658) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-45079c919740e19b82b941714aa4ccc383a112ca8df30778c06f3658*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "3807\t0.35\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "4268\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 5f8470ab97b09fc9e8814a6429a9c9f0c46a8b8c01000000, name=_ray_fit, pid=4268, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.952923), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 45079c919740e19b82b941714aa4ccc383a112ca8df30778c06f3658) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-45079c919740e19b82b941714aa4ccc383a112ca8df30778c06f3658*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "3807\t0.35\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "4268\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 605.11s of the 1205.29s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.96%)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4341, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4341, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 595.74s of the 1195.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.13%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 39594f5ed783002d0cf7dd1a343918206bc90d3101000000, name=_ray_fit, pid=4442, memory used=0.06GB) was running was 3.60GB / 3.76GB (0.957956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cfe29fc611d1f61f0c203a6114f8aaafd29a14eef0e687602f48d051) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-cfe29fc611d1f61f0c203a6114f8aaafd29a14eef0e687602f48d051*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4340\t0.19\tray::IDLE\n",
      "4341\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "4442\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 39594f5ed783002d0cf7dd1a343918206bc90d3101000000, name=_ray_fit, pid=4442, memory used=0.06GB) was running was 3.60GB / 3.76GB (0.957956), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: cfe29fc611d1f61f0c203a6114f8aaafd29a14eef0e687602f48d051) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-cfe29fc611d1f61f0c203a6114f8aaafd29a14eef0e687602f48d051*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.60\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4340\t0.19\tray::IDLE\n",
      "4341\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "4442\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-03-29 19:35:23,397\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_ray_fit()\u001b[39m (pid=4340, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 591.71s of the 1191.90s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 50.28% memory usage per fold, 50.28%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=50.28%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: f23cb125fe41c809386441813f5251faf85d936c01000000, name=_ray_fit, pid=4507, memory used=0.05GB) was running was 3.64GB / 3.76GB (0.968129), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4eb8d8fb688fa3dada34c736365cd77382c000be6d0786b62723f3c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d4eb8d8fb688fa3dada34c736365cd77382c000be6d0786b62723f3c*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4340\t0.19\tray::IDLE\n",
      "4341\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: f23cb125fe41c809386441813f5251faf85d936c01000000, name=_ray_fit, pid=4507, memory used=0.05GB) was running was 3.64GB / 3.76GB (0.968129), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4eb8d8fb688fa3dada34c736365cd77382c000be6d0786b62723f3c) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d4eb8d8fb688fa3dada34c736365cd77382c000be6d0786b62723f3c*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4340\t0.19\tray::IDLE\n",
      "4341\t0.19\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 590.53s of the 1190.72s of remaining time.\n",
      "\tWarning: Model is expected to require 205.47% of available memory...\n",
      "\tNot enough memory to train ExtraTrees_r42_BAG_L1... Skipping this model.\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 589.64s of the 1189.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.51%)\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ef0c1d5db96064368176219d7536b54f7c11d73e01000000, name=_ray_fit, pid=4679, memory used=0.15GB) was running was 3.57GB / 3.76GB (0.950238), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8169d403b4fee566d4f1d0eb99d07bbc45ba2e8e9f42b20e0251c50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d8169d403b4fee566d4f1d0eb99d07bbc45ba2e8e9f42b20e0251c50*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4584\t0.26\tray::IDLE\n",
      "4679\t0.15\tray::_ray_fit\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ef0c1d5db96064368176219d7536b54f7c11d73e01000000, name=_ray_fit, pid=4679, memory used=0.15GB) was running was 3.57GB / 3.76GB (0.950238), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d8169d403b4fee566d4f1d0eb99d07bbc45ba2e8e9f42b20e0251c50) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d8169d403b4fee566d4f1d0eb99d07bbc45ba2e8e9f42b20e0251c50*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "4584\t0.26\tray::IDLE\n",
      "4679\t0.15\tray::_ray_fit\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 571.22s of the 1171.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=8.29%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 6d351fa23c4ed0cd11992fd3c93463f18d2c8c5e01000000, name=_ray_fit, pid=5043, memory used=0.08GB) was running was 3.58GB / 3.76GB (0.951533), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 64d8bdec59520c25434438d7fd0d653211964c0a4346dbfdfe89f73d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-64d8bdec59520c25434438d7fd0d653211964c0a4346dbfdfe89f73d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "5043\t0.08\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 6d351fa23c4ed0cd11992fd3c93463f18d2c8c5e01000000, name=_ray_fit, pid=5043, memory used=0.08GB) was running was 3.58GB / 3.76GB (0.951533), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 64d8bdec59520c25434438d7fd0d653211964c0a4346dbfdfe89f73d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-64d8bdec59520c25434438d7fd0d653211964c0a4346dbfdfe89f73d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "5043\t0.08\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 539.77s of the 1139.95s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.27% memory usage per fold, 73.10%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=18.27%)\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 7f5e11810415c4c7c4e6363c03bfdeb37b24896201000000, name=_ray_fit, pid=5101, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.953226), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c0295f450bdef7b99afb6b7ca8b94837a71328416b939a7dd9f471b2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c0295f450bdef7b99afb6b7ca8b94837a71328416b939a7dd9f471b2*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5101\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 7f5e11810415c4c7c4e6363c03bfdeb37b24896201000000, name=_ray_fit, pid=5101, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.953226), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c0295f450bdef7b99afb6b7ca8b94837a71328416b939a7dd9f471b2) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c0295f450bdef7b99afb6b7ca8b94837a71328416b939a7dd9f471b2*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5101\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 538.24s of the 1138.43s of remaining time.\n",
      "\tWarning: Model is expected to require 131.74% of available memory...\n",
      "\tNot enough memory to train RandomForest_r195_BAG_L1... Skipping this model.\n",
      "2025-03-29 19:36:18,432\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 536.67s of the 1136.85s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.94% memory usage per fold, 43.75%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=10.94%)\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: b56d38113222a15605a3ddb0bfd6935cfe4958cf01000000, name=_ray_fit, pid=5163, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952429), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e22a8fe818090766401e735c21141f4ba7ac4bd198678c84b2b23) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d32e22a8fe818090766401e735c21141f4ba7ac4bd198678c84b2b23*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5163\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: b56d38113222a15605a3ddb0bfd6935cfe4958cf01000000, name=_ray_fit, pid=5163, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952429), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d32e22a8fe818090766401e735c21141f4ba7ac4bd198678c84b2b23) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-d32e22a8fe818090766401e735c21141f4ba7ac4bd198678c84b2b23*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5163\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 534.83s of the 1135.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.69%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ab391c0bb2c4b817a66e237c9fa03eb233828a4a01000000, name=_ray_fit, pid=5209, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.950719), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fa023d8a3807c4aa9883fa8e6b8699481fedca42f1240fb85d6ade80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-fa023d8a3807c4aa9883fa8e6b8699481fedca42f1240fb85d6ade80*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.33\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5209\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ab391c0bb2c4b817a66e237c9fa03eb233828a4a01000000, name=_ray_fit, pid=5209, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.950719), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fa023d8a3807c4aa9883fa8e6b8699481fedca42f1240fb85d6ade80) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-fa023d8a3807c4aa9883fa8e6b8699481fedca42f1240fb85d6ade80*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.62\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "4714\t0.33\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "5209\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 533.19s of the 1133.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.74%)\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a12b5b5aa2a50455d76eefd6b6359c63fd865b6f01000000, name=_ray_fit, pid=5442, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.951691), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 443e46e9141f931c63ca159c60dc7fdac474bdb946ac3750c73be67b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-443e46e9141f931c63ca159c60dc7fdac474bdb946ac3750c73be67b*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "5345\t0.16\t\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "5442\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: a12b5b5aa2a50455d76eefd6b6359c63fd865b6f01000000, name=_ray_fit, pid=5442, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.951691), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 443e46e9141f931c63ca159c60dc7fdac474bdb946ac3750c73be67b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-443e46e9141f931c63ca159c60dc7fdac474bdb946ac3750c73be67b*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "5345\t0.16\t\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "5442\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 517.05s of the 1117.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.48%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r30_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: e1e2a19c1c54f57d184f0779a62222c8b04bf23f01000000, name=_ray_fit, pid=7957, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.950683), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fcac29b2be575867cbb706b3a66294d877d18306d7bf95af538fd445) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-fcac29b2be575867cbb706b3a66294d877d18306d7bf95af538fd445*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "7809\t0.34\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "7957\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: e1e2a19c1c54f57d184f0779a62222c8b04bf23f01000000, name=_ray_fit, pid=7957, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.950683), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: fcac29b2be575867cbb706b3a66294d877d18306d7bf95af538fd445) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-fcac29b2be575867cbb706b3a66294d877d18306d7bf95af538fd445*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "7809\t0.34\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "7957\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 575.27s of remaining time.\n",
      "\tEnsemble Weights: {'ExtraTreesEntr_BAG_L1': 0.375, 'RandomForestEntr_BAG_L1': 0.312, 'RandomForestGini_BAG_L1': 0.188, 'KNeighborsDist_BAG_L1': 0.125}\n",
      "\t0.5954\t = Validation score   (average_precision)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 574.67s of the 574.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.88%)\n",
      "\tWarning: Exception caused LightGBMXT_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=7956, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=7956, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 564.95s of the 564.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.07%)\n",
      "\tWarning: Exception caused LightGBM_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=8088, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=8088, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 555.84s of the 555.79s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 56 due to low memory. Expected memory usage reduced from 79.26% -> 15.0% of available memory...\n",
      "\t0.6144\t = Validation score   (average_precision)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 549.72s of the 549.68s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 87 due to low memory. Expected memory usage reduced from 51.51% -> 15.0% of available memory...\n",
      "\t0.6149\t = Validation score   (average_precision)\n",
      "\t7.16s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 541.51s of the 541.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.56%)\n",
      "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 0f964a40652e6be02fd449cb9b789a1c381e6b0701000000, name=_ray_fit, pid=8789, memory used=0.12GB) was running was 3.58GB / 3.76GB (0.951899), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 400a0d2dd1b78b7931608ec510792d0aa9eb9491a8ec9dfe64919b99) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-400a0d2dd1b78b7931608ec510792d0aa9eb9491a8ec9dfe64919b99*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "8549\t0.26\tray::IDLE\n",
      "8789\t0.12\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 0f964a40652e6be02fd449cb9b789a1c381e6b0701000000, name=_ray_fit, pid=8789, memory used=0.12GB) was running was 3.58GB / 3.76GB (0.951899), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 400a0d2dd1b78b7931608ec510792d0aa9eb9491a8ec9dfe64919b99) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-400a0d2dd1b78b7931608ec510792d0aa9eb9491a8ec9dfe64919b99*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.61\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "8549\t0.26\tray::IDLE\n",
      "8789\t0.12\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 458.39s of the 458.35s of remaining time.\n",
      "\tWarning: Model is expected to require 234.22% of available memory...\n",
      "\tNot enough memory to train ExtraTreesGini_BAG_L2... Skipping this model.\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 457.22s of the 457.18s of remaining time.\n",
      "\tWarning: Model is expected to require 234.59% of available memory...\n",
      "\tNot enough memory to train ExtraTreesEntr_BAG_L2... Skipping this model.\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 456.33s of the 456.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.40%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: b553812c98c6de4cd2b2231cc27c95b79c133e9101000000, name=_ray_fit, pid=9952, memory used=0.33GB) was running was 3.58GB / 3.76GB (0.952056), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 44c5f0d176380924bb9efc9bd244a985ba4709be45365fcd9ec8057d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-44c5f0d176380924bb9efc9bd244a985ba4709be45365fcd9ec8057d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "9952\t0.33\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "689\t0.05\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/_private/log_monitor.py --sessi...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: b553812c98c6de4cd2b2231cc27c95b79c133e9101000000, name=_ray_fit, pid=9952, memory used=0.33GB) was running was 3.58GB / 3.76GB (0.952056), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 44c5f0d176380924bb9efc9bd244a985ba4709be45365fcd9ec8057d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-44c5f0d176380924bb9efc9bd244a985ba4709be45365fcd9ec8057d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "9952\t0.33\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "689\t0.05\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/_private/log_monitor.py --sessi...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 339.91s of the 339.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.56%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 0e24e455344e405ef718e61842e64854e8c6ae8e01000000, name=_ray_fit, pid=10259, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f602ce536d14299a4cdf12185fa323c29847e47fd24a568848c619a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-1f602ce536d14299a4cdf12185fa323c29847e47fd24a568848c619a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10188\t0.15\tray::IDLE\n",
      "10189\t0.15\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 0e24e455344e405ef718e61842e64854e8c6ae8e01000000, name=_ray_fit, pid=10259, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952566), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 1f602ce536d14299a4cdf12185fa323c29847e47fd24a568848c619a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-1f602ce536d14299a4cdf12185fa323c29847e47fd24a568848c619a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10188\t0.15\tray::IDLE\n",
      "10189\t0.15\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 330.25s of the 330.20s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=7.95%)\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 1f50ef3c8abb0bb05b53a972c616a8f0c786a6f401000000, name=_ray_fit, pid=10320, memory used=0.05GB) was running was 3.57GB / 3.76GB (0.950252), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c271b24abfc3e20a8c1739beef2b68e4246d7248039dbcbb644312fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c271b24abfc3e20a8c1739beef2b68e4246d7248039dbcbb644312fc*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10188\t0.15\tray::IDLE\n",
      "10189\t0.15\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 1f50ef3c8abb0bb05b53a972c616a8f0c786a6f401000000, name=_ray_fit, pid=10320, memory used=0.05GB) was running was 3.57GB / 3.76GB (0.950252), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c271b24abfc3e20a8c1739beef2b68e4246d7248039dbcbb644312fc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-c271b24abfc3e20a8c1739beef2b68e4246d7248039dbcbb644312fc*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10188\t0.15\tray::IDLE\n",
      "10189\t0.15\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 328.45s of the 328.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=7.46%)\n",
      "\tWarning: Exception caused LightGBMLarge_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=10384, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=10384, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 318.62s of the 318.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=8.64%)\n",
      "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 3e81f260fa646901cf313f8c5af52e8555285f6601000000, name=_ray_fit, pid=10661, memory used=0.09GB) was running was 3.58GB / 3.76GB (0.950883), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 10d2fd6a0acad65f05f692866d62e09018def155c0230e43ebd5ed0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-10d2fd6a0acad65f05f692866d62e09018def155c0230e43ebd5ed0f*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10487\t0.26\tray::IDLE\n",
      "10661\t0.09\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 3e81f260fa646901cf313f8c5af52e8555285f6601000000, name=_ray_fit, pid=10661, memory used=0.09GB) was running was 3.58GB / 3.76GB (0.950883), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 10d2fd6a0acad65f05f692866d62e09018def155c0230e43ebd5ed0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-10d2fd6a0acad65f05f692866d62e09018def155c0230e43ebd5ed0f*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "10487\t0.26\tray::IDLE\n",
      "10661\t0.09\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 292.63s of the 292.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.08%)\n",
      "2025-03-29 19:50:27,491\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused NeuralNetTorch_r79_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 76a7989ea77646444cfc71f125510ea3d84c317d01000000, name=_ray_fit, pid=11902, memory used=0.32GB) was running was 3.58GB / 3.76GB (0.95309), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 124c09949a397f5ccd21895880cd997b48c04e7837e2bc9299c00e7e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-124c09949a397f5ccd21895880cd997b48c04e7837e2bc9299c00e7e*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "11902\t0.32\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "639\t0.05\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py ...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 76a7989ea77646444cfc71f125510ea3d84c317d01000000, name=_ray_fit, pid=11902, memory used=0.32GB) was running was 3.58GB / 3.76GB (0.95309), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 124c09949a397f5ccd21895880cd997b48c04e7837e2bc9299c00e7e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-124c09949a397f5ccd21895880cd997b48c04e7837e2bc9299c00e7e*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "11902\t0.32\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "639\t0.05\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/autoscaler/_private/monitor.py ...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 182.13s of the 182.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.19%)\n",
      "\tWarning: Exception caused LightGBM_r131_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=12146, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=12146, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 172.80s of the 172.75s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.23% memory usage per fold, 40.92%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=10.23%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r191_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 454977df39010f166471f1fa7c34db1bf78a52a501000000, name=_ray_fit, pid=12219, memory used=0.34GB) was running was 3.58GB / 3.76GB (0.951585), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a90434c10aecfc5f1954f31e7e48ad5be7e8441262af702345c35d9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-a90434c10aecfc5f1954f31e7e48ad5be7e8441262af702345c35d9a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "12219\t0.34\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13025\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 454977df39010f166471f1fa7c34db1bf78a52a501000000, name=_ray_fit, pid=12219, memory used=0.34GB) was running was 3.58GB / 3.76GB (0.951585), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a90434c10aecfc5f1954f31e7e48ad5be7e8441262af702345c35d9a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-a90434c10aecfc5f1954f31e7e48ad5be7e8441262af702345c35d9a*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "12219\t0.34\tray::_ray_fit\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13025\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 133.39s of the 133.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.17%)\n",
      "2025-03-29 19:53:06,649\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused CatBoost_r9_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: cced38c760755e482a7df5f7735b9deb2074446401000000, name=_ray_fit, pid=13358, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.952337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 23d01ea181622a1c77040ee5ad01a60cc5a3a077ebb4b39de88917a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-23d01ea181622a1c77040ee5ad01a60cc5a3a077ebb4b39de88917a4*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13054\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13358\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: cced38c760755e482a7df5f7735b9deb2074446401000000, name=_ray_fit, pid=13358, memory used=0.06GB) was running was 3.58GB / 3.76GB (0.952337), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 23d01ea181622a1c77040ee5ad01a60cc5a3a077ebb4b39de88917a4) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-23d01ea181622a1c77040ee5ad01a60cc5a3a077ebb4b39de88917a4*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13054\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13358\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: LightGBM_r96_BAG_L2 ... Training model for up to 96.96s of the 96.92s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.75%)\n",
      "\tWarning: Exception caused LightGBM_r96_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=13417, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=13417, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L2 ... Training model for up to 87.61s of the 87.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.77%)\n",
      "\tWarning: Exception caused NeuralNetTorch_r22_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 5566d4bdaafb6247712db13a5db60831e5ece46e01000000, name=_ray_fit, pid=13923, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.953305), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 15106a3eaaed8b02a651e1ce7d29fdbc1fea05784b08091ffc9ba99f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-15106a3eaaed8b02a651e1ce7d29fdbc1fea05784b08091ffc9ba99f*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13923\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 5566d4bdaafb6247712db13a5db60831e5ece46e01000000, name=_ray_fit, pid=13923, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.953305), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 15106a3eaaed8b02a651e1ce7d29fdbc1fea05784b08091ffc9ba99f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-15106a3eaaed8b02a651e1ce7d29fdbc1fea05784b08091ffc9ba99f*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.63\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13923\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r33_BAG_L2 ... Training model for up to 59.58s of the 59.54s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 1 folds in parallel instead (Estimated 48.85% memory usage per fold, 48.85%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=1, gpus=0, memory=48.85%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_r33_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 11c7e8bfa20144a5b661dbac22f35dd9862c4f5201000000, name=_ray_fit, pid=13952, memory used=0.06GB) was running was 3.62GB / 3.76GB (0.963199), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2e5a9d8adeb0d596cf6448a7af73bceeca5e67ac47c0c109244df41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-e2e5a9d8adeb0d596cf6448a7af73bceeca5e67ac47c0c109244df41*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13952\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 11c7e8bfa20144a5b661dbac22f35dd9862c4f5201000000, name=_ray_fit, pid=13952, memory used=0.06GB) was running was 3.62GB / 3.76GB (0.963199), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e2e5a9d8adeb0d596cf6448a7af73bceeca5e67ac47c0c109244df41) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-e2e5a9d8adeb0d596cf6448a7af73bceeca5e67ac47c0c109244df41*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "13952\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: ExtraTrees_r42_BAG_L2 ... Training model for up to 58.10s of the 58.05s of remaining time.\n",
      "\tWarning: Model is expected to require 174.35% of available memory...\n",
      "\tNot enough memory to train ExtraTrees_r42_BAG_L2... Skipping this model.\n",
      "Fitting model: CatBoost_r137_BAG_L2 ... Training model for up to 56.96s of the 56.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=9.80%)\n",
      "\tWarning: Exception caused CatBoost_r137_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: be0b39187bc481f5bf4677f8faeeafa03bb83cd101000000, name=_ray_fit, pid=13953, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952954), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b38cd7b80d64a2c5f82e31a5a930a06ac14fd48eb55e0332edeb2cfd) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-b38cd7b80d64a2c5f82e31a5a930a06ac14fd48eb55e0332edeb2cfd*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "13953\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: be0b39187bc481f5bf4677f8faeeafa03bb83cd101000000, name=_ray_fit, pid=13953, memory used=0.05GB) was running was 3.58GB / 3.76GB (0.952954), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: b38cd7b80d64a2c5f82e31a5a930a06ac14fd48eb55e0332edeb2cfd) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-b38cd7b80d64a2c5f82e31a5a930a06ac14fd48eb55e0332edeb2cfd*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "13953\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L2 ... Training model for up to 55.21s of the 55.17s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.69% memory usage per fold, 66.75%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=16.69%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r102_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: d21e581285429cdfea6f7b732a8fc4662914b9f701000000, name=_ray_fit, pid=14030, memory used=0.05GB) was running was 3.63GB / 3.76GB (0.964267), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a1db390a21e4790cc3f07ad8a9fb629691075b5f08d6857b3b679d3e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-a1db390a21e4790cc3f07ad8a9fb629691075b5f08d6857b3b679d3e*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "14030\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: d21e581285429cdfea6f7b732a8fc4662914b9f701000000, name=_ray_fit, pid=14030, memory used=0.05GB) was running was 3.63GB / 3.76GB (0.964267), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: a1db390a21e4790cc3f07ad8a9fb629691075b5f08d6857b3b679d3e) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-a1db390a21e4790cc3f07ad8a9fb629691075b5f08d6857b3b679d3e*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.31\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "14030\t0.05\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: CatBoost_r13_BAG_L2 ... Training model for up to 53.41s of the 53.36s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.65% memory usage per fold, 45.30%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=22.65%)\n",
      "\tWarning: Exception caused CatBoost_r13_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 7ee9fb0ad6618ab6412bf59409eb766b7d8ac3c101000000, name=_ray_fit, pid=14093, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954989), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0a16eff5d9a42b20a6b488c0d6901cddbedea7f62de64f533a85ba7d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-0a16eff5d9a42b20a6b488c0d6901cddbedea7f62de64f533a85ba7d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "14093\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: 7ee9fb0ad6618ab6412bf59409eb766b7d8ac3c101000000, name=_ray_fit, pid=14093, memory used=0.06GB) was running was 3.59GB / 3.76GB (0.954989), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0a16eff5d9a42b20a6b488c0d6901cddbedea7f62de64f533a85ba7d) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-0a16eff5d9a42b20a6b488c0d6901cddbedea7f62de64f533a85ba7d*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.64\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "13517\t0.32\tray::IDLE\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "14093\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-03-29 19:54:23,875\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: RandomForest_r195_BAG_L2 ... Training model for up to 51.45s of the 51.40s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 98 due to low memory. Expected memory usage reduced from 45.72% -> 15.0% of available memory...\n",
      "\t0.6148\t = Validation score   (average_precision)\n",
      "\t17.89s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L2 ... Training model for up to 32.45s of the 32.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=7.56%)\n",
      "\tWarning: Exception caused LightGBM_r188_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=14129, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 871, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::_ray_fit()\u001b[39m (pid=14129, ip=169.255.255.2)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 283, in _fit\n",
      "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 134, in train_lgb_model\n",
      "    return lgb.train(**train_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/lightgbm/engine.py\", line 317, in train\n",
      "    cb(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 255, in _callback\n",
      "    _mem_early_stop()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/tabular/models/lgb/callbacks.py\", line 162, in _mem_early_stop\n",
      "    % (best_iter[0] + 1, \"\\t\".join([_format_eval_result(x, show_stdv=False) for x in best_score_list[0]])),\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L2 ... Training model for up to 23.17s of the 23.13s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.88% memory usage per fold, 47.52%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=11.88%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_r145_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: f3ab9f5bd43502f7aed0114f014064b852fc1ad701000000, name=_ray_fit, pid=14225, memory used=0.06GB) was running was 3.57GB / 3.76GB (0.950622), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2feea2b52a928ef919734edf68240913504456c024459f6dc576e2db) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-2feea2b52a928ef919734edf68240913504456c024459f6dc576e2db*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.66\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "14129\t0.23\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "14225\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: f3ab9f5bd43502f7aed0114f014064b852fc1ad701000000, name=_ray_fit, pid=14225, memory used=0.06GB) was running was 3.57GB / 3.76GB (0.950622), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2feea2b52a928ef919734edf68240913504456c024459f6dc576e2db) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-2feea2b52a928ef919734edf68240913504456c024459f6dc576e2db*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.66\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "14129\t0.23\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "14225\t0.06\tray::IDLE\n",
      "174\t0.06\t/opt/conda/bin/python3.11 /opt/conda/lib/python3.11/site-packages/jedi/inference/compiled/subprocess...\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: XGBoost_r89_BAG_L2 ... Training model for up to 20.63s of the 20.58s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.48% memory usage per fold, 41.93%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=10.48%)\n",
      "2025-03-29 19:54:59,746\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tWarning: Exception caused XGBoost_r89_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tTask was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ac74b0a2c02b5e276adb0c6372a4d9b8d6b2ec1301000000, name=_ray_fit, pid=14392, memory used=0.06GB) was running was 3.63GB / 3.76GB (0.965547), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: afbcf507f5e8efc5a353df2dbb074862ae731cc2e3b43ccf4f2767d7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-afbcf507f5e8efc5a353df2dbb074862ae731cc2e3b43ccf4f2767d7*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.65\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "14327\t0.16\tray::IDLE\n",
      "14261\t0.16\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "14392\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 2691, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/ray/_private/worker.py\", line 873, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 169.255.255.2, ID: 64aa2973db018f4ffd4445f20c74d50696015ba34cdf6663d61d6e54) where the task (task ID: ac74b0a2c02b5e276adb0c6372a4d9b8d6b2ec1301000000, name=_ray_fit, pid=14392, memory used=0.06GB) was running was 3.63GB / 3.76GB (0.965547), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: afbcf507f5e8efc5a353df2dbb074862ae731cc2e3b43ccf4f2767d7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 169.255.255.2`. To see the logs of the worker, use `ray logs worker-afbcf507f5e8efc5a353df2dbb074862ae731cc2e3b43ccf4f2767d7*out -ip 169.255.255.2. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "446\t0.65\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "136\t0.58\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "361\t0.30\t/opt/conda/bin/python -m ipykernel_launcher -f /home/sagemaker-user/.local/share/jupyter/runtime/ker...\n",
      "154\t0.29\t/opt/conda/bin/python3.11 -m pylsp\n",
      "47\t0.28\t/opt/conda/bin/python3.11 /opt/conda/bin/jupyter-lab --ip 0.0.0.0 --port 8888 --ServerApp.base_url=/...\n",
      "14327\t0.16\tray::IDLE\n",
      "14261\t0.16\tray::IDLE\n",
      "640\t0.07\t/opt/conda/bin/python /opt/conda/lib/python3.11/site-packages/ray/dashboard/dashboard.py --host=127....\n",
      "733\t0.07\t/opt/conda/bin/python -u /opt/conda/lib/python3.11/site-packages/ray/dashboard/agent.py --node-ip-ad...\n",
      "14392\t0.06\tray::IDLE\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 7.64s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestGini_BAG_L2': 0.35, 'RandomForestEntr_BAG_L2': 0.25, 'ExtraTreesEntr_BAG_L1': 0.2, 'RandomForest_r195_BAG_L2': 0.2}\n",
      "\t0.6286\t = Validation score   (average_precision)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1793.54s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3824.5 rows/s (7040 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/sagemaker-user/AutogluonModels/ag-20250329_192515\")\n"
     ]
    }
   ],
   "source": [
    "# AutoGluon predictor\n",
    "predictor = TabularPredictor(\n",
    "    label='offer_completed_after_view',\n",
    "    problem_type='binary',\n",
    "    eval_metric='average_precision'  # Use PR_AUC since it is an unbalanced binary classification problem\n",
    ").fit(\n",
    "    train_data=train_dataset,\n",
    "    tuning_data=val_dataset,\n",
    "    presets='best_quality',\n",
    "    time_limit=60 * 30, # 30 minutes of time limit\n",
    "    use_bag_holdout=True,\n",
    "    verbosity=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./data/predictor.pickle', 'wb') as handle:\n",
    "    pickle.dump(predictor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./data/predictor.pickle', 'rb') as handle:\n",
    "    predictor = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T16:39:50.634545Z",
     "start_time": "2023-05-19T16:39:50.571342Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val        eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L3   0.628582  average_precision       1.840786  39.645676                0.005607           0.992467            3       True         10\n",
      "1   RandomForestEntr_BAG_L2   0.614943  average_precision       1.224754  15.568836                0.344004           7.156762            2       True          8\n",
      "2  RandomForest_r195_BAG_L2   0.614829  average_precision       1.231750  26.302462                0.351001          17.890388            2       True          9\n",
      "3   RandomForestGini_BAG_L2   0.614351  average_precision       1.140173  13.606060                0.259423           5.193985            2       True          7\n",
      "4       WeightedEnsemble_L2   0.595407  average_precision       0.790996   8.823151                0.004061           0.559655            2       True          6\n",
      "5     ExtraTreesEntr_BAG_L1   0.581995  average_precision       0.215866   1.547739                0.215866           1.547739            1       True          5\n",
      "6   RandomForestEntr_BAG_L1   0.581081  average_precision       0.279598   3.619978                0.279598           3.619978            1       True          4\n",
      "7   RandomForestGini_BAG_L1   0.580351  average_precision       0.223524   2.999130                0.223524           2.999130            1       True          3\n",
      "8     KNeighborsDist_BAG_L1   0.442907  average_precision       0.067947   0.096648                0.067947           0.096648            1       True          2\n",
      "9     KNeighborsUnif_BAG_L1   0.412897  average_precision       0.093815   0.148579                0.093815           0.148579            1       True          1\n",
      "Number of models trained: 10\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_RF', 'StackerEnsembleModel_KNN', 'WeightedEnsembleModel', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 5 | ['age', 'income', 'reward', 'difficulty', 'duration']\n",
      "('int', [])       : 1 | ['membership_days']\n",
      "('int', ['bool']) : 9 | ['gender_F', 'gender_M', 'gender_O', 'mobile', 'social', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "# View the summary of the fit\n",
    "fit_summary = predictor.fit_summary(show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best models are an ensemble, a neural netowrk and LightGBM with extra trees parameter (extra randomized trees). For simplicity and interpretably we will use LightGBM, which is available in Sagemaker also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
